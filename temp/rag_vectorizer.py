#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGå‘é‡åŒ–å’Œå­˜å‚¨æ¨¡å—
ä½¿ç”¨FAISSä½œä¸ºå‘é‡æ•°æ®åº“ï¼Œsentence-transformersä½œä¸ºåµŒå…¥æ¨¡å‹
"""

import json
import os
import pickle
from datetime import datetime
from typing import Dict, List, Tuple

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer


class RAGVectorizer:
    """
    RAGå‘é‡åŒ–å™¨ï¼Œè´Ÿè´£æ–‡æ¡£çš„å‘é‡åŒ–ã€å­˜å‚¨å’Œæ£€ç´¢
    """
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", index_path: str = "faiss_index", use_local_model: bool = True):
        """
        åˆå§‹åŒ–RAGå‘é‡åŒ–å™¨
        
        Args:
            model_name (str): sentence-transformersæ¨¡å‹åç§°
            index_path (str): FAISSç´¢å¼•ä¿å­˜è·¯å¾„
            use_local_model (bool): æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        """
        print(f"ğŸš€ åˆå§‹åŒ–RAGå‘é‡åŒ–å™¨...")
        print(f"ğŸ“Š åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if use_local_model:
            local_model_path = f"./models/{model_name}"
            if os.path.exists(local_model_path):
                model_path = local_model_path
                print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")
            else:
                print(f"âš ï¸ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {local_model_path}")
                print(f"ğŸ’¡ è¯·åˆ›å»ºç›®å½•å¹¶ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°: {local_model_path}")
                print("éœ€è¦çš„æ–‡ä»¶:")
                required_files = [
                    "config.json", "pytorch_model.bin", "tokenizer.json",
                    "tokenizer_config.json", "vocab.txt", "special_tokens_map.json",
                    "modules.json", "sentence_bert_config.json", "config_sentence_transformers.json"
                ]
                for file in required_files:
                    print(f"  - {file}")
                model_path = model_name  # å›é€€åˆ°åœ¨çº¿ä¸‹è½½
        else:
            model_path = model_name
        
        # åˆå§‹åŒ–sentence-transformersæ¨¡å‹
        try:
            if not use_local_model or not os.path.exists(local_model_path):
                # å¤„ç†SSLé—®é¢˜ç”¨äºåœ¨çº¿ä¸‹è½½
                import ssl
                ssl._create_default_https_context = ssl._create_unverified_context
            
            self.model = SentenceTransformer(model_path, device='cpu')
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ›´ç®€å•çš„é…ç½®
            try:
                import ssl
                ssl._create_default_https_context = ssl._create_unverified_context
                self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device='cpu')
                print("âœ… ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: paraphrase-MiniLM-L6-v2")
            except:
                print("âŒ æ— æ³•åŠ è½½ä»»ä½•sentence-transformersæ¨¡å‹")
                print("ğŸ’¡ å»ºè®®ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: python simple_rag_test.py")
                raise
        
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        print(f"âœ… æ¨¡å‹å‘é‡ç»´åº¦: {self.embedding_dim}")
        
        # FAISSç´¢å¼•ç›¸å…³
        self.index = None
        self.index_path = index_path
        self.metadata_path = f"{index_path}_metadata.pkl"
        
        # å­˜å‚¨æ–‡æ¡£å—çš„å…ƒæ•°æ®
        self.chunks_metadata = []
        self.document_chunks = []
        
        # åˆ›å»ºFAISSç´¢å¼•
        self._create_index()
    
    def _create_index(self):
        """åˆ›å»ºFAISSç´¢å¼•"""
        print(f"ğŸ”§ åˆ›å»ºFAISSç´¢å¼•...")
        # ä½¿ç”¨L2è·ç¦»çš„å¹³é¢ç´¢å¼•ï¼Œé€‚åˆä¸­å°è§„æ¨¡æ•°æ®
        self.index = faiss.IndexFlatL2(self.embedding_dim)
        print(f"âœ… FAISSç´¢å¼•åˆ›å»ºå®Œæˆ")
    
    def add_overlap_to_chunks(self, chunks: List[Dict], overlap_chars: int = 100) -> List[Dict]:
        """
        ä¸ºæ–‡æ¡£å—æ·»åŠ é‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
        
        Args:
            chunks (List[Dict]): åŸå§‹æ–‡æ¡£å—åˆ—è¡¨
            overlap_chars (int): é‡å å­—ç¬¦æ•°
        
        Returns:
            List[Dict]: æ·»åŠ é‡å åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        print(f"ğŸ”„ ä¸ºæ–‡æ¡£å—æ·»åŠ é‡å  (é‡å å­—ç¬¦æ•°: {overlap_chars})")
        
        if not chunks:
            return chunks
        
        overlapped_chunks = []
        
        for i, chunk in enumerate(chunks):
            content = chunk['content']
            metadata = chunk['metadata'].copy()
            
            # æ·»åŠ å‰ç½®é‡å ï¼ˆæ¥è‡ªå‰ä¸€ä¸ªchunkçš„ç»“å°¾ï¼‰
            prefix = ""
            if i > 0:
                prev_content = chunks[i-1]['content']
                if len(prev_content) >= overlap_chars:
                    prefix = "..." + prev_content[-overlap_chars:]
                else:
                    prefix = "..." + prev_content
            
            # æ·»åŠ åç½®é‡å ï¼ˆæ¥è‡ªåä¸€ä¸ªchunkçš„å¼€å¤´ï¼‰
            suffix = ""
            if i < len(chunks) - 1:
                next_content = chunks[i+1]['content']
                if len(next_content) >= overlap_chars:
                    suffix = next_content[:overlap_chars] + "..."
                else:
                    suffix = next_content + "..."
            
            # æ„å»ºå¸¦é‡å çš„å†…å®¹
            overlapped_content = f"{prefix} {content} {suffix}".strip()
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata['has_overlap'] = True
            metadata['original_content'] = content
            metadata['overlap_chars'] = overlap_chars
            
            overlapped_chunk = {
                'chunk_id': chunk['chunk_id'],
                'content': overlapped_content,
                'metadata': metadata
            }
            
            overlapped_chunks.append(overlapped_chunk)
        
        print(f"âœ… é‡å æ·»åŠ å®Œæˆï¼Œå¤„ç†äº† {len(overlapped_chunks)} ä¸ªæ–‡æ¡£å—")
        return overlapped_chunks
    
    def embed_chunks(self, chunks: List[Dict]) -> Tuple[np.ndarray, List[Dict]]:
        """
        å°†æ–‡æ¡£å—å‘é‡åŒ–
        
        Args:
            chunks (List[Dict]): æ–‡æ¡£å—åˆ—è¡¨
        
        Returns:
            Tuple[np.ndarray, List[Dict]]: (å‘é‡çŸ©é˜µ, å…ƒæ•°æ®åˆ—è¡¨)
        """
        print(f"ğŸ”„ å¼€å§‹å‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æ¡£å—...")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = [chunk['content'] for chunk in chunks]
        
        # æ‰¹é‡å‘é‡åŒ–
        model_name = getattr(self.model, 'model_name', getattr(self.model, '_model_name', 'sentence-transformer'))
        print(f"ğŸ“Š ä½¿ç”¨ {model_name} è¿›è¡Œå‘é‡åŒ–...")
        embeddings = self.model.encode(texts, 
                                     show_progress_bar=True,
                                     batch_size=32,
                                     convert_to_numpy=True)
        
        print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œç”Ÿæˆ {embeddings.shape} å‘é‡çŸ©é˜µ")
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata_list = []
        for chunk in chunks:
            metadata = chunk['metadata'].copy()
            metadata['chunk_id'] = chunk['chunk_id']
            metadata['embedding_timestamp'] = datetime.now().isoformat()
            metadata_list.append(metadata)
        
        return embeddings, metadata_list
    
    def add_documents_to_index(self, rag_content: Dict, overlap_chars: int = 100):
        """
        å°†RAGå†…å®¹æ·»åŠ åˆ°FAISSç´¢å¼•
        
        Args:
            rag_content (Dict): RAGæ ¼å¼çš„æ–‡æ¡£å†…å®¹
            overlap_chars (int): é‡å å­—ç¬¦æ•°
        """
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_name = rag_content['document_metadata'].get('file_name', 'Unknown')
        file_path = rag_content['document_metadata'].get('file_path', 'Unknown')
        
        print(f"\nğŸ“š å¼€å§‹å¤„ç†æ–‡æ¡£: {file_name}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {file_path}")
        
        chunks = rag_content['chunks']
        if not chunks:
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£å—ï¼Œè·³è¿‡å¤„ç†")
            return
        
        # æ·»åŠ é‡å 
        overlapped_chunks = self.add_overlap_to_chunks(chunks, overlap_chars)
        
        # å‘é‡åŒ–
        embeddings, metadata_list = self.embed_chunks(overlapped_chunks)
        
        # æ·»åŠ åˆ°FAISSç´¢å¼•
        print(f"ğŸ’¾ å°†å‘é‡æ·»åŠ åˆ°FAISSç´¢å¼•...")
        self.index.add(embeddings.astype('float32'))
        
        # ä¿å­˜å…ƒæ•°æ®
        current_index = len(self.chunks_metadata)
        for i, (chunk, metadata) in enumerate(zip(overlapped_chunks, metadata_list)):
            metadata['faiss_index'] = current_index + i
            self.chunks_metadata.append(metadata)
            self.document_chunks.append(chunk)
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(overlapped_chunks)} ä¸ªå‘é‡åˆ°ç´¢å¼•")
        print(f"ğŸ“Š å½“å‰ç´¢å¼•æ€»é‡: {self.index.ntotal} ä¸ªå‘é‡")
    
    def save_index(self):
        """ä¿å­˜FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        print(f"\nğŸ’¾ ä¿å­˜FAISSç´¢å¼•åˆ°: {self.index_path}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(os.path.dirname(self.index_path) if os.path.dirname(self.index_path) else ".", exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, f"{self.index_path}.faiss")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_to_save = {
            'chunks_metadata': self.chunks_metadata,
            'document_chunks': self.document_chunks,
            'model_name': getattr(self.model, 'model_name', getattr(self.model, '_model_name', 'sentence-transformer')),
            'embedding_dim': self.embedding_dim,
            'created_at': datetime.now().isoformat(),
            'total_vectors': self.index.ntotal
        }
        
        with open(self.metadata_path, 'wb') as f:
            pickle.dump(metadata_to_save, f)
        
        print(f"âœ… ç´¢å¼•ä¿å­˜å®Œæˆ")
        print(f"  - FAISSç´¢å¼•: {self.index_path}.faiss")
        print(f"  - å…ƒæ•°æ®: {self.metadata_path}")
    
    def load_index(self) -> bool:
        """
        åŠ è½½å·²ä¿å­˜çš„FAISSç´¢å¼•å’Œå…ƒæ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        faiss_file = f"{self.index_path}.faiss"
        
        if not os.path.exists(faiss_file) or not os.path.exists(self.metadata_path):
            print(f"âš ï¸ æœªæ‰¾åˆ°å·²ä¿å­˜çš„ç´¢å¼•æ–‡ä»¶")
            return False
        
        try:
            print(f"ğŸ“‚ åŠ è½½FAISSç´¢å¼•: {faiss_file}")
            self.index = faiss.read_index(faiss_file)
            
            print(f"ğŸ“‚ åŠ è½½å…ƒæ•°æ®: {self.metadata_path}")
            with open(self.metadata_path, 'rb') as f:
                metadata = pickle.load(f)
            
            self.chunks_metadata = metadata['chunks_metadata']
            self.document_chunks = metadata['document_chunks']
            
            print(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ")
            print(f"  - å‘é‡æ•°é‡: {self.index.ntotal}")
            print(f"  - æ–‡æ¡£å—æ•°é‡: {len(self.chunks_metadata)}")
            print(f"  - åˆ›å»ºæ—¶é—´: {metadata.get('created_at', 'Unknown')}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def search(self, query: str, top_k: int = 5, file_filter: str = None) -> List[Dict]:
        """
        åœ¨ç´¢å¼•ä¸­æœç´¢ç›¸å…³æ–‡æ¡£å—
        
        Args:
            query (str): æŸ¥è¯¢æ–‡æœ¬
            top_k (int): è¿”å›çš„ç»“æœæ•°é‡
            file_filter (str): å¯é€‰çš„æ–‡ä»¶åè¿‡æ»¤å™¨
        
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if self.index.ntotal == 0:
            print("âš ï¸ ç´¢å¼•ä¸ºç©ºï¼Œæ— æ³•æœç´¢")
            return []
        
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: '{query}' (top_k={top_k})")
        if file_filter:
            print(f"ğŸ“ æ–‡ä»¶è¿‡æ»¤: {file_filter}")
        
        # å¯¹æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        
        # åœ¨FAISSä¸­æœç´¢ - å…ˆæœç´¢æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤
        search_k = top_k * 3 if file_filter else top_k
        distances, indices = self.index.search(query_embedding.astype('float32'), search_k)
        
        # æ„å»ºç»“æœ
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx >= 0 and idx < len(self.chunks_metadata):
                metadata = self.chunks_metadata[idx]
                
                # å¦‚æœæœ‰æ–‡ä»¶è¿‡æ»¤å™¨ï¼Œæ£€æŸ¥æ–‡ä»¶å
                if file_filter:
                    file_name = metadata.get('file_name', '')
                    if file_filter.lower() not in file_name.lower():
                        continue
                
                result = {
                    'rank': len(results) + 1,
                    'score': float(distance),
                    'similarity': 1 / (1 + float(distance)),  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    'chunk': self.document_chunks[idx],
                    'metadata': metadata
                }
                results.append(result)
                
                # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœå°±åœæ­¢
                if len(results) >= top_k:
                    break
        
        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
        return results
    
    def get_file_statistics(self) -> Dict:
        """è·å–æŒ‰æ–‡ä»¶åˆ†ç»„çš„ç»Ÿè®¡ä¿¡æ¯"""
        file_stats = {}
        
        for metadata in self.chunks_metadata:
            file_name = metadata.get('file_name', 'Unknown')
            if file_name not in file_stats:
                file_stats[file_name] = {
                    'chunk_count': 0,
                    'file_path': metadata.get('file_path', 'Unknown'),
                    'pages': set()
                }
            
            file_stats[file_name]['chunk_count'] += 1
            if 'page_number' in metadata:
                file_stats[file_name]['pages'].add(metadata['page_number'])
        
        # è½¬æ¢é¡µé¢é›†åˆä¸ºåˆ—è¡¨
        for file_name in file_stats:
            file_stats[file_name]['pages'] = sorted(list(file_stats[file_name]['pages']))
            file_stats[file_name]['page_count'] = len(file_stats[file_name]['pages'])
        
        return file_stats
    
    def print_file_statistics(self):
        """æ‰“å°æŒ‰æ–‡ä»¶åˆ†ç»„çš„ç»Ÿè®¡ä¿¡æ¯"""
        file_stats = self.get_file_statistics()
        
        print(f"\nğŸ“ æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯:")
        for file_name, stats in file_stats.items():
            print(f"  ğŸ“„ {file_name}:")
            print(f"    - æ–‡æ¡£å—æ•°: {stats['chunk_count']}")
            print(f"    - é¡µé¢æ•°: {stats['page_count']}")
            print(f"    - æ–‡ä»¶è·¯å¾„: {stats['file_path']}")
    
    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_vectors': self.index.ntotal if self.index else 0,
            'embedding_dimension': self.embedding_dim,
            'total_chunks': len(self.chunks_metadata),
            'model_name': getattr(self.model, 'model_name', getattr(self.model, '_model_name', 'sentence-transformer')),
            'index_path': self.index_path
        }
    
    def print_stats(self):
        """æ‰“å°ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        print(f"\nğŸ“Š RAGå‘é‡åº“ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - å‘é‡æ€»æ•°: {stats['total_vectors']}")
        print(f"  - å‘é‡ç»´åº¦: {stats['embedding_dimension']}")
        print(f"  - æ–‡æ¡£å—æ•°: {stats['total_chunks']}")
        print(f"  - åµŒå…¥æ¨¡å‹: {stats['model_name']}")
        print(f"  - ç´¢å¼•è·¯å¾„: {stats['index_path']}")
        
        # åŒæ—¶æ‰“å°æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        self.print_file_statistics()


def load_rag_content(json_file: str) -> Dict:
    """
    ä»JSONæ–‡ä»¶åŠ è½½RAGå†…å®¹
    
    Args:
        json_file (str): JSONæ–‡ä»¶è·¯å¾„
    
    Returns:
        Dict: RAGå†…å®¹
    """
    print(f"ğŸ“‚ åŠ è½½RAGå†…å®¹: {json_file}")
    with open(json_file, 'r', encoding='utf-8') as f:
        content = json.load(f)
    print(f"âœ… åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(content.get('chunks', []))} ä¸ªæ–‡æ¡£å—")
    return content


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºRAGå‘é‡åŒ–æµç¨‹"""
    print("ğŸš€ RAGå‘é‡åŒ–ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–å‘é‡åŒ–å™¨
    vectorizer = RAGVectorizer(
        model_name="all-MiniLM-L6-v2",
        index_path="./rag_index/faiss_index"
    )
    
    # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
    if vectorizer.load_index():
        print("ğŸ“‚ ä½¿ç”¨å·²æœ‰ç´¢å¼•")
    else:
        print("ğŸ†• åˆ›å»ºæ–°ç´¢å¼•")
        
        # åŠ è½½PDFæå–çš„RAGå†…å®¹
        # ä½¿ç”¨tmall_pnp_method_sop_rag_ready.jsonä½œä¸ºç¤ºä¾‹
        rag_files = [
            "tmall_pnp_method_sop_rag_ready.json"
        ]
        
        for rag_file in rag_files:
            if os.path.exists(rag_file):
                try:
                    rag_content = load_rag_content(rag_file)
                    vectorizer.add_documents_to_index(rag_content, overlap_chars=100)
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ä»¶ {rag_file} å¤±è´¥: {e}")
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {rag_file}")
        
        # ä¿å­˜ç´¢å¼•
        vectorizer.save_index()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    vectorizer.print_stats()
    
    # æ¼”ç¤ºæœç´¢åŠŸèƒ½
    print("\n" + "=" * 50)
    print("ğŸ” æœç´¢æ¼”ç¤º")
    
    test_queries = [
        "weekly split units å¦‚ä½•è®¡ç®—",
        "monthly promo price è®¾ç½®æ–¹æ³•", 
        "daily units å¤„ç†æµç¨‹",
        "ä¿ƒé”€ä»·æ ¼è®¡ç®—",
        "weekly promo price bau"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = vectorizer.search(query, top_k=3)
        
        for result in results:
            print(f"  {result['rank']}. ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
            print(f"     æ–‡ä»¶: {result['metadata'].get('file_name', 'Unknown')}")
            print(f"     é¡µé¢: {result['metadata']['page_number']}")
            print(f"     å†…å®¹: {result['chunk']['content'][:100]}...")
            if 'original_content' in result['metadata']:
                print(f"     åŸæ–‡: {result['metadata']['original_content'][:80]}...")
            print()


if __name__ == "__main__":
    main()
